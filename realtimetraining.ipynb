{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.fc = nn.Linear(40, 10)  # 40 inputs, 10 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_40384\\1909700107.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the updated model\n",
    "model = EmotionRecognitionModel()\n",
    "\n",
    "# Define the path to the saved model\n",
    "model_path = 'D:\\\\PROJECTS\\\\trained_model.pth'\n",
    "\n",
    "# Load the state_dict into the model\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(\"Model loaded successfully.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion map created from dataset: {0: 'TESS Toronto emotional speech set data'}\n"
     ]
    }
   ],
   "source": [
    "# Function to create an emotion map dynamically based on the folder names in your dataset\n",
    "def create_emotion_map(dataset_path):\n",
    "    emotions = sorted(os.listdir(dataset_path))  # Get the folder names (emotions)\n",
    "    emotion_map = {i: emotion for i, emotion in enumerate(emotions)}\n",
    "    return emotion_map\n",
    "\n",
    "# Create emotion map based on the dataset structure\n",
    "dataset_path = 'D:\\\\PROJECTS\\\\archive'\n",
    "emotion_map = create_emotion_map(dataset_path)\n",
    "print(f'Emotion map created from dataset: {emotion_map}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the audio file\n",
    "def preprocess_audio(file_path):\n",
    "    # Load audio file\n",
    "    data, sample_rate = librosa.load(file_path, sr=16000)  # Adjust sample rate if necessary\n",
    "    # Extract features (MFCC)\n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)\n",
    "    # Take the mean of MFCC features along the time axis\n",
    "    mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict emotion using the PyTorch model\n",
    "def predict_emotion(file_path):\n",
    "    features = preprocess_audio(file_path)\n",
    "    features = torch.tensor(features).unsqueeze(0)  # Reshape for model input as a tensor\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        output = model(features.float())  # Forward pass through the model\n",
    "        predicted_emotion = torch.argmax(output, dim=1).item()  # Get the predicted emotion\n",
    "    return emotion_map[predicted_emotion]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPROJECTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124marchive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTESS Toronto emotional speech set data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvoice.wav\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Path to your uploaded audio file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Recognize the emotion of the uploaded audio\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m emotion \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_emotion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe emotion in the audio is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[93], line 8\u001b[0m, in \u001b[0;36mpredict_emotion\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(features\u001b[38;5;241m.\u001b[39mfloat())  \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     predicted_emotion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Get the predicted emotion\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43memotion_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_emotion\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Upload and predict emotion\n",
    "file_path = r'D:\\PROJECTS\\archive\\TESS Toronto emotional speech set data\\voice.wav' # Path to your uploaded audio file\n",
    "\n",
    "# Recognize the emotion of the uploaded audio\n",
    "emotion = predict_emotion(file_path)\n",
    "print(f'The emotion in the audio is: {emotion}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
